{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "238bc361",
   "metadata": {},
   "source": [
    "# Minimal example for the package\n",
    "\n",
    "This notebook was generated from `run_topicmodels.py`. It contains example runs for the `topicmodels` package (SPF, PF, CSPF, TBIP, ...).\n",
    "\n",
    "Notes: keep the working directory so that relative paths to `data/10k_amazon.csv` work (run from the repository root)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a91e10",
   "metadata": {},
   "source": [
    "## Prelims\n",
    "\n",
    "For each of the following models we need to:\n",
    "- load the data\n",
    "- create the DTM\n",
    "- save the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c42a4616-6a42-4570-bdb4-671bd6043636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.11/site-packages (4.67.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c83413c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "# Prelims \n",
    " \n",
    "# ---- Load data ----\n",
    "df1 = pd.read_csv(\"data/10k_amazon.csv\")\n",
    "\n",
    "# ---- Define keywords ----\n",
    "pets = [\"dog\",\"cat\", \"litter\", \"cats\", \"dogs\", \"food\", \"box\", \"collar\", \"water\", \"pet\"]\n",
    "toys = [\"toy\", \"game\", \"play\", \"fun\", \"old\", \"son\", \"year\", \"loves\", \"kids\", \"daughter\"]\n",
    "beauty = [\"hair\", \"skin\", \"product\", \"color\", \"scent\", \"smell\", \"used\", \"dry\", \"using\", \"products\"]\n",
    "baby = [\"baby\", \"seat\", \"diaper\", \"diapers\", \"stroller\", \"bottles\", \"son\", \"pump\", \"gate\", \"months\"]\n",
    "health = [\"product\", \"like\", \"razor\", \"shave\", \"time\", \"day\", \"shaver\", \"better\", \"work\", \"years\"]\n",
    "grocery = [\"tea\", \"taste\", \"flavor\", \"coffee\", \"sauce\", \"chocolate\", \"sugar\", \"eat\", \"sweet\", \"delicious\"]\n",
    "\n",
    "keywords = {\"pet supplies\": pets, \"toys games\": toys, \"beauty\": beauty, \"baby products\": baby,\n",
    "            \"health personal care\": health, \"grocery gourmet food\": grocery}\n",
    "\n",
    "# --- Create corpus ---\n",
    "cv = CountVectorizer(stop_words='english', min_df = 2)\n",
    "cv.fit(df1[\"Text\"])\n",
    "counts = sparse.csr_matrix(cv.transform(df1[\"Text\"]), dtype = np.float32)\n",
    "vocab = cv.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4175a7a2",
   "metadata": {},
   "source": [
    "## SPF\n",
    "\n",
    "Minimal example for the Seeded Poisson Factorization model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05fdf776",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Init loss:   576.3346; Avg loss (last 10 iter):   247.2767:   7%|â–‹         | 69/1000 [00:59<13:23,  1.16it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m tm1 = SPF(counts, vocab, keywords, residual_topics = \u001b[32m0\u001b[39m, batch_size = \u001b[32m1024\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# ---- Run inference -----\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m estimated_params = \u001b[43mtm1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# ---- Inspect results ----\u001b[39;00m\n\u001b[32m     10\u001b[39m estimated_params\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/01_Coding/02_GitHub/topicmodels_package/poisson_topicmodels/models/numpyro_model.py:141\u001b[39m, in \u001b[36mNumpyroModel.train_step\u001b[39m\u001b[34m(self, num_steps, lr, random_seed)\u001b[39m\n\u001b[32m    138\u001b[39m pbar = tqdm(\u001b[38;5;28mrange\u001b[39m(num_steps))\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     Y_batch, D_batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrngs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcounts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m     svi_state, loss = svi_batch_update(svi_state, Y_batch=Y_batch, d_batch=D_batch)\n\u001b[32m    143\u001b[39m     loss = loss / \u001b[38;5;28mself\u001b[39m.D\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/01_Coding/02_GitHub/topicmodels_package/poisson_topicmodels/models/numpyro_model.py:78\u001b[39m, in \u001b[36mNumpyroModel._get_batch\u001b[39m\u001b[34m(self, rng, Y)\u001b[39m\n\u001b[32m     76\u001b[39m D_batch = random.choice(rng, jnp.arange(\u001b[38;5;28mself\u001b[39m.D), shape=(\u001b[38;5;28mself\u001b[39m.batch_size,))\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# Y_batch = jax.device_put(jnp.array(Y[D_batch].toarray()), jax.devices(\"cpu\")[0])\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m Y_batch = jnp.array(\u001b[43mY\u001b[49m\u001b[43m[\u001b[49m\u001b[43mD_batch\u001b[49m\u001b[43m]\u001b[49m.toarray())\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# Ensure the shape of Y_batch is (batch_size, V)\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m Y_batch.shape == (\n\u001b[32m     82\u001b[39m     \u001b[38;5;28mself\u001b[39m.batch_size,\n\u001b[32m     83\u001b[39m     \u001b[38;5;28mself\u001b[39m.V,\n\u001b[32m     84\u001b[39m ), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mShape mismatch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mY_batch.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m != (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.batch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.V\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/01_Coding/02_GitHub/topicmodels_package/.venv/lib/python3.11/site-packages/scipy/sparse/_index.py:30\u001b[39m, in \u001b[36mIndexMixin.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     index, new_shape = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m     \u001b[38;5;66;03m# 1D array\u001b[39;00m\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(index) == \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/01_Coding/02_GitHub/topicmodels_package/.venv/lib/python3.11/site-packages/scipy/sparse/_index.py:244\u001b[39m, in \u001b[36mIndexMixin._validate_indices\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    239\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\n\u001b[32m    240\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mIndexing with sparse matrices is not supported \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    241\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mexcept boolean indexing where matrix and index \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    242\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mare equal shapes.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# dense array\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m         index_1st.append(np.asarray(idx))\n\u001b[32m    245\u001b[39m         prelim_ndim += \u001b[32m1\u001b[39m\n\u001b[32m    246\u001b[39m ellip_slices = (\u001b[38;5;28mself\u001b[39m.ndim - prelim_ndim) * [\u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m)]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from poisson_topicmodels import SPF\n",
    "\n",
    "# ---- Initialize TM package ----\n",
    "tm1 = SPF(counts, vocab, keywords, residual_topics = 0, batch_size = 1024)\n",
    "\n",
    "# ---- Run inference -----\n",
    "estimated_params = tm1.train_step(num_steps = 1000, lr = 0.1)\n",
    "\n",
    "# ---- Inspect results ----\n",
    "estimated_params\n",
    "topics, e_theta = tm1.return_topics()\n",
    "beta = tm1.return_beta()\n",
    "top_words = tm1.return_top_words_per_topic(n = 10)\n",
    "\n",
    "# --- See loss within inherited metrics object ---\n",
    "tm1.Metrics.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a00c88",
   "metadata": {},
   "source": [
    "## PF\n",
    "\n",
    "Minimal example for the Poisson Factorization model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acab7e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from poisson_topicmodels import PF\n",
    "\n",
    "tm2 = PF(counts, vocab, num_topics = 10, batch_size = 1024)\n",
    "estimated_params = tm2.train_step(num_steps = 100, lr =0.01)\n",
    "topics, e_theta = tm2.return_topics()\n",
    "betas = tm2.return_beta()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fb5362",
   "metadata": {},
   "source": [
    "## CSPF\n",
    "\n",
    "Minimal example for the Covariate-informed Seeded Poisson Factorization model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372175b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from poisson_topicmodels import CSPF\n",
    "\n",
    "category0 = [\"grocery gourmet food\", \"toys games\"]\n",
    "covariable = df1['Cat1'].apply(lambda x: 0 if x in category0 else 1)\n",
    "print(covariable[0:10])\n",
    "print(df1['Cat1'].head(10))\n",
    "\n",
    "X_design_matrix = pd.DataFrame({'intercept' : np.repeat(1, len(df1)), 'var_infromative' : covariable})\n",
    "\n",
    "tm3 = CSPf(counts, vocab, keywords, residual_topics = 2, batch_size = 1024, X_design_matrix = X_design_matrix)\n",
    "estimated_params = tm3.train_step(num_steps = 1000, lr = 0.01)\n",
    "topics, e_theta = tm3.return_topics()\n",
    "betas = tm3.return_beta()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17659c8",
   "metadata": {},
   "source": [
    "## CPF\n",
    "\n",
    "Minimal example for the Covariate-informed Poisson Factorization model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac85c775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from poisson_topicmodels import CPF\n",
    "# tm3 = CPF(counts, vocab, num_topics = 5, batch_size = 1024, X_design_matrix = X_design_matrix)\n",
    "# svi_batch, svi_state = tm3.train_step(num_steps = 100, lr = 0.01)\n",
    "# estimated_params = svi_batch.get_params(svi_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0d9c3b",
   "metadata": {},
   "source": [
    "## TBIP\n",
    "\n",
    "Minimal example for Text-based ideal point model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40fc81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from poisson_topicmodels import TBIP\n",
    "df1['speaker'] = np.random.choice(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K'], size=len(df1), replace=True)\n",
    "tm4 = TBIP(counts, vocab, num_topics = 10, authors = df1.speaker, batch_size = 1024)\n",
    "estimated_params = tm4.train_step(num_steps = 1000, lr = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc3c61b",
   "metadata": {},
   "source": [
    "## TVTBIP\n",
    "\n",
    "Minimal example for the Time-varying Text-based ideal point model.\n",
    "\n",
    "\n",
    "Already implemented. Add minimal example here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topicmodels-package-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
